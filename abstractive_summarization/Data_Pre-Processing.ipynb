{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Dataset \n",
    "\n",
    "Dataset source: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data # 0\n",
      "Processing data # 10000\n",
      "Processing data # 20000\n",
      "Processing data # 30000\n",
      "Processing data # 40000\n",
      "Processing data # 50000\n",
      "Processing data # 60000\n",
      "Processing data # 70000\n",
      "Processing data # 80000\n",
      "Processing data # 90000\n",
      "Processing data # 100000\n",
      "Processing data # 110000\n",
      "Processing data # 120000\n",
      "Processing data # 130000\n",
      "Processing data # 140000\n",
      "Processing data # 150000\n",
      "Processing data # 160000\n",
      "Processing data # 170000\n",
      "Processing data # 180000\n",
      "Processing data # 190000\n",
      "Processing data # 200000\n",
      "Processing data # 210000\n",
      "Processing data # 220000\n",
      "Processing data # 230000\n",
      "Processing data # 240000\n",
      "Processing data # 250000\n",
      "Processing data # 260000\n",
      "Processing data # 270000\n",
      "Processing data # 280000\n",
      "Processing data # 290000\n",
      "Processing data # 300000\n",
      "Processing data # 310000\n",
      "Processing data # 320000\n",
      "Processing data # 330000\n",
      "\n",
      "# of Data: 337465\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import string\n",
    "\n",
    "summaries = []\n",
    "texts = []\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()                           #lower() 로 대문자를 소문자로 변경\n",
    "    printable = set(string.printable)\n",
    "    text = \"\".join(list(filter(lambda x: x in printable, text))) #filter funny characters, if any.  text에 있으면서, printable list에 있는 x를 선택\n",
    "    return text\n",
    "\n",
    "text_max_len = 500\n",
    "text_min_len = 25\n",
    "summary_max_len = 30\n",
    "vocab2idx = {}\n",
    "\n",
    "#Data from https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "with open('Dataset/Reviews.csv') as csvfile:    #text와 summary가 함께 제공되는 csv 파일\n",
    "                                                #예) summary : Good Quality Dog Food\n",
    "                                                #   text : I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
    "    \n",
    "    Reviews = csv.DictReader(csvfile)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    for row in Reviews:\n",
    "        \n",
    "        text = row['Text']                #review text 하나씩 text에 저장\n",
    "        summary = row['Summary']          #review summary 하나씩 summary에 저장\n",
    "        \n",
    "        if len(text) <= text_max_len and len(text) >= text_min_len and len(summary) <= summary_max_len:   #위에서 지정한 길이에 부합한다면\n",
    "            #print(i)\n",
    "\n",
    "            clean_text = clean(text)          #review text를 전처리\n",
    "            clean_summary = clean(summary)    #review summary를 전처리\n",
    "            \n",
    "            tokenized_summary = word_tokenize(clean_summary)   #nltk에서 import한 word_tokenize로 tokenize\n",
    "            tokenized_text = word_tokenize(clean_text)\n",
    "            \n",
    "            # BUILD VOCABULARY\n",
    "            \n",
    "            for word in tokenized_text:(\n",
    "                if word not in vocab2idx:\n",
    "                    vocab2idx[word]=len(vocab2idx)    #tokenize된 단어들을 순서대로 0번부터 index를 주면서 vocab2idx에 입력, 중복은 없도록\n",
    "                                                      # 예) {(i:0), (have:1), (bought:2)....}\n",
    "            for word in tokenized_summary:\n",
    "                if word not in vocab2idx:\n",
    "                    vocab2idx[word]=len(vocab2idx)\n",
    "                    \n",
    "            ## ________________\n",
    "\n",
    "            summaries.append(tokenized_summary)       #최종적으로 summaries에는 csv파일에 있는 모든 summary를 설정한 사이즈에 맞고, clean하고, tokenize된 데이터가 들어간다\n",
    "            texts.append(tokenized_text)\n",
    "\n",
    "            if i%10000==0:\n",
    "                print(\"Processing data # {}\".format(i))\n",
    "\n",
    "            i+=1\n",
    "\n",
    "print(\"\\n# of Data: {}\".format(len(texts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE CLEANED & TOKENIZED TEXT: \n",
      "\n",
      "['we', 'got', 'a', 'box', 'of', 'these', 'to', 'put', 'in', 'gift', 'bags', 'for', 'our', 'sons', \"'\", 'birthday', 'party', '.', 'what', 'can', 'i', 'say', ':', 'it', \"'s\", 'a', 'box', 'of', 'gummi', 'pizzas', '.', 'i', 'would', \"n't\", 'recommend', 'eating', 'a', 'whole', 'box', 'yourself', ',', 'but', 'sharing', 'them', 'with', 'a', 'bunch', 'of', 'kids', 'works', 'great.', '<', 'br', '/', '>', 'from', 'a', 'gummi', 'candy', 'standpoint', ',', 'they', \"'re\", 'not', 'the', 'yummiest', 'kind', 'of', 'gummi', 'candy', ',', 'but', 'they', 'have', 'a', 'bit', 'more', 'novelty', 'with', 'the', 'pizza', 'appearance', '.']\n",
      "\n",
      "\n",
      "SAMPLE CLEANED & TOKENIZED SUMMARY: \n",
      "\n",
      "['yummi']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0,len(texts)-1)\n",
    "\n",
    "print(\"SAMPLE CLEANED & TOKENIZED TEXT: \\n\\n{}\\n\\n\".format(texts[index]))\n",
    "print(\"SAMPLE CLEANED & TOKENIZED SUMMARY: \\n\\n{}\\n\".format(summaries[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings\n",
    "\n",
    "Loading pre-trained GloVe embeddings. Source of Data: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Loaded.\n",
      "Vocabulary Size: 43544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab = []\n",
    "embd = []\n",
    "special_tags = ['<UNK>','<PAD>','<EOS>']         #unk:unkown token / pad: 정해진 길이를 채우기 위한 padding / eos:end of sentence\n",
    "\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab2embd = {}\n",
    "    \n",
    "    with open(filename, 'rt', encoding='UTF8') as infile:   #unicode decode error발생하여 encoding='UTF8' 추가     \n",
    "        for line in infile:\n",
    "            row = line.strip().split(' ')                   #strip 함수로 양쪽 끝에 있는 공백과 \\n 기호를 삭제 / split 함수로 공백 기준으로 line을 나눈 list 생성\n",
    "            word = row[0].lower()                           #lower 함수로 소문자를 대문자로 바꾸기\n",
    "            if word not in vocab2embd:\n",
    "                vocab2embd[word]=np.asarray(row[1:],np.float32)      #ㅠㅠ파일이 너무 커서 안열림. 어떻게 생겼는지 알고싶은데\n",
    "\n",
    "    print('Embedding Loaded.')\n",
    "    return vocab2embd\n",
    "\n",
    "vocab2embd = loadEmbeddings('Embeddings/glove.6B.100d.txt')\n",
    "\n",
    "for word in vocab2idx:\n",
    "    if word in vocab2embd:\n",
    "        vocab.append(word)                              #embedding되는 word만 vocab에 넣음\n",
    "        embd.append(vocab2embd[word])                   #하나의 word에 대하여 vaocab, embd list의 같은 index위치에 word와 embedding이 있는 것.\n",
    "        \n",
    "for special_tag in special_tags:\n",
    "    vocab.append(special_tag)\n",
    "    embd.append(np.random.rand(len(embd[0]),))\n",
    "    \n",
    "vocab2idx = {word:idx for idx,word in enumerate(vocab)} #이제 word가 key / index가 value  위에서 중복이 없도록 word를 입력했기 때문에 문제없음\n",
    "embd = np.asarray(embd,np.float32)   \n",
    "\n",
    "print(\"Vocabulary Size: {}\".format(len(vocab2idx)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43543\n"
     ]
    }
   ],
   "source": [
    "print(vocab2idx['<EOS>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_texts=[]\n",
    "vec_summaries=[]\n",
    "\n",
    "for text,summary in zip(texts,summaries):\n",
    "    # Replace out of vocab words with index for '<UNK>' tag\n",
    "    #get함수는 딕셔너리에서 ()안에 있는 key값에 대응하는 value값을 return\n",
    "    #a.get(x, y) : a라는 딕셔너리에서 y라는 key값에 대응하는 value값을 return하도록 하는데, 없으면 y를 return\n",
    "    vec_texts.append([vocab2idx.get(word,vocab2idx['<UNK>']) for word in text])  #word를 int값으로 vectorize\n",
    "    vec_summaries.append([vocab2idx.get(word,vocab2idx['<UNK>']) for word in summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "\n",
    "texts_idx = [idx for idx in range(len(vec_texts))]\n",
    "random.shuffle(texts_idx)\n",
    "\n",
    "vec_texts = [vec_texts[idx] for idx in texts_idx]\n",
    "vec_summaries = [vec_summaries[idx] for idx in texts_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use first 10000 data for testing, the next 10000 data for validation, and rest for training\n",
    "\n",
    "test_summaries = vec_summaries[0:10000]\n",
    "test_texts = vec_texts[0:10000]\n",
    "\n",
    "val_summaries = vec_summaries[10000:20000]\n",
    "val_texts = vec_texts[10000:20000]\n",
    "\n",
    "train_summaries = vec_summaries[20000:]\n",
    "train_texts = vec_texts[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket And Batch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_and_batch(texts,summaries,batch_size=32):\n",
    "    \n",
    "    # Sort summaries and texts according to the length of text\n",
    "    # (So that texts with similar lengths tend to remain in the same batch and thus require less padding)\n",
    "    \n",
    "    text_lens = [len(text) for text in texts]\n",
    "    sortedidx = np.flip(np.argsort(text_lens),axis=0) #text_lens를 작은 값부터 큰 값 순서대로 sort한 후, 행을 기준으로 뒤집음. \n",
    "                                                      #즉, 큰 값이 앞으로, 작은 값이 뒤로 \n",
    "    texts=[texts[idx] for idx in sortedidx]          #문장이 긴 text가 앞에, 문장이 짧은 text가 뒤에\n",
    "    summaries=[summaries[idx] for idx in sortedidx]\n",
    "    \n",
    "    batches_text=[]\n",
    "    batches_summary=[]\n",
    "    batches_true_text_len = []\n",
    "    batches_true_summary_len = []\n",
    "    \n",
    "    i=0\n",
    "    while i < (len(texts)-batch_size):               #batch size에 맞춰서 padding\n",
    "        \n",
    "        max_len = len(texts[i])                       \n",
    "        \n",
    "        batch_text=[]\n",
    "        batch_summary=[]\n",
    "        batch_true_text_len=[]\n",
    "        batch_true_summary_len=[]\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            \n",
    "            padded_text = texts[i+j]\n",
    "            padded_summary = summaries[i+j]\n",
    "            \n",
    "            batch_true_text_len.append(len(texts[i+j]))\n",
    "            batch_true_summary_len.append(len(summaries[i+j])+1)\n",
    "     \n",
    "            while len(padded_text) < max_len:\n",
    "                padded_text.append(vocab2idx['<PAD>'])\n",
    "\n",
    "            padded_summary.append(vocab2idx['<EOS>']) #End of Sentence Marker\n",
    "            while len(padded_summary) < summary_max_len+1:\n",
    "                padded_summary.append(vocab2idx['<PAD>'])\n",
    "            \n",
    "        \n",
    "            batch_text.append(padded_text)\n",
    "            batch_summary.append(padded_summary)\n",
    "        \n",
    "        batches_text.append(batch_text)\n",
    "        batches_summary.append(batch_summary)\n",
    "        batches_true_text_len.append(batch_true_text_len)\n",
    "        batches_true_summary_len.append(batch_true_summary_len)\n",
    "        \n",
    "        i+=batch_size\n",
    "        \n",
    "    return batches_text, batches_summary, batches_true_text_len, batches_true_summary_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_text, train_batches_summary, train_batches_true_text_len, train_batches_true_summary_len \\\n",
    "= bucket_and_batch(train_texts, train_summaries)\n",
    "\n",
    "val_batches_text, val_batches_summary, val_batches_true_text_len, val_batches_true_summary_len \\\n",
    "= bucket_and_batch(val_texts, val_summaries)\n",
    "\n",
    "test_batches_text, test_batches_summary, test_batches_true_text_len, test_batches_true_summary_len \\\n",
    "= bucket_and_batch(test_texts, test_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "d = {}\n",
    "\n",
    "d[\"vocab\"] = vocab2idx\n",
    "d[\"embd\"] = embd.tolist()\n",
    "d[\"train_batches_text\"] = train_batches_text\n",
    "d[\"test_batches_text\"] = test_batches_text\n",
    "d[\"val_batches_text\"] = val_batches_text\n",
    "d[\"train_batches_summary\"] = train_batches_summary\n",
    "d[\"test_batches_summary\"] = test_batches_summary\n",
    "d[\"val_batches_summary\"] = val_batches_summary\n",
    "d[\"train_batches_true_text_len\"] = train_batches_true_text_len\n",
    "d[\"val_batches_true_text_len\"] = val_batches_true_text_len\n",
    "d[\"test_batches_true_text_len\"] = test_batches_true_text_len\n",
    "d[\"train_batches_true_summary_len\"] = train_batches_true_summary_len\n",
    "d[\"val_batches_true_summary_len\"] = val_batches_true_summary_len\n",
    "d[\"test_batches_true_summary_len\"] = test_batches_true_summary_len\n",
    "\n",
    "with open('Processed_Data/Amazon_Reviews_Processed.json', 'w') as outfile:\n",
    "    json.dump(d, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
